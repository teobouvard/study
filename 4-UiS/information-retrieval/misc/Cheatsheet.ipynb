{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def class_metrics(actual, predicted, binary=True):\n",
    "    conf_mat = metrics.confusion_matrix(actual, predicted)\n",
    "    avg_acc = metrics.accuracy_score(actual, predicted)\n",
    "    cosine_sim = metrics.pairwise.cosine_similarity(np.array(actual).reshape(1, -1), np.array(predicted).reshape(1, -1))\n",
    "    print(f'confusion matrix : \\n{conf_mat}')\n",
    "    print(f'cosine similarity : {cosine_sim}')\n",
    "    print(f'average accuracy : {avg_acc}')\n",
    "    if binary:\n",
    "        precision = metrics.precision_score(actual, predicted)\n",
    "        recall = metrics.recall_score(actual, predicted)\n",
    "        f1 = metrics.f1_score(actual, predicted)\n",
    "        print(f'precision : {precision}')\n",
    "        print(f'recall : {recall}')\n",
    "        print(f'f1-score : {f1}')\n",
    "    else:\n",
    "        micro_prec = metrics.precision_score(actual, predicted, average='micro')\n",
    "        micro_recall = metrics.recall_score(actual, predicted, average='micro')\n",
    "        micro_f1 = metrics.f1_score(actual, predicted, average='micro')\n",
    "        macro_prec = metrics.precision_score(actual, predicted, average='macro')\n",
    "        macro_recall = metrics.recall_score(actual, predicted, average='macro')\n",
    "        macro_f1 = metrics.f1_score(actual, predicted, average='macro')\n",
    "        print(f'micro precision : {micro_prec}')\n",
    "        print(f'micro recall : {micro_recall}')\n",
    "        print(f'micro f1 : {micro_f1}')\n",
    "        print(f'macro precision : {macro_prec}')\n",
    "        print(f'macro recall : {macro_recall}')\n",
    "        print(f'macro f1 : {macro_f1}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix : \n",
      "[[2 0 0 0]\n",
      " [1 1 1 0]\n",
      " [1 1 2 0]\n",
      " [0 1 1 1]]\n",
      "cosine similarity : [[0.94324222]]\n",
      "average accuracy : 0.5\n",
      "micro precision : 0.5\n",
      "micro recall : 0.5\n",
      "micro f1 : 0.5\n",
      "macro precision : 0.5833333333333333\n",
      "macro recall : 0.5416666666666666\n",
      "macro f1 : 0.5\n"
     ]
    }
   ],
   "source": [
    "actual = [1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4]\n",
    "predicted = [1, 1, 1, 2, 3, 2, 3, 1, 3, 4, 2, 3]\n",
    "class_metrics(actual, predicted, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix : \n",
      "[[3 3]\n",
      " [1 3]]\n",
      "cosine similarity : [[0.61237244]]\n",
      "average accuracy : 0.6\n",
      "precision : 0.5\n",
      "recall : 0.75\n",
      "f1-score : 0.6\n"
     ]
    }
   ],
   "source": [
    "actual = [0, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n",
    "predicted = [1, 1, 1, 1, 0, 0, 1, 0, 0, 1]\n",
    "class_metrics(actual, predicted, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix : \n",
      "[[1 1]\n",
      " [1 2]]\n",
      "cosine similarity : [[0.66666667]]\n",
      "average accuracy : 0.6\n",
      "precision : 0.6666666666666666\n",
      "recall : 0.6666666666666666\n",
      "f1-score : 0.6666666666666666\n",
      "confusion matrix : \n",
      "[[1 1]\n",
      " [2 1]]\n",
      "cosine similarity : [[0.40824829]]\n",
      "average accuracy : 0.4\n",
      "precision : 0.5\n",
      "recall : 0.3333333333333333\n",
      "f1-score : 0.4\n"
     ]
    }
   ],
   "source": [
    "actual = [1, 0, 1, 1, 0]\n",
    "predictedA = [1, 1, 1, 0, 0]\n",
    "predictedB = [0, 0, 0, 1, 1]\n",
    "class_metrics(actual, predictedA, binary=True)\n",
    "class_metrics(actual, predictedB, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "$$sim_{cos}(x,y) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}||~||\\mathbf{y}||} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sqrt{\\sum_{i=1}^n x_i^2} \\sqrt{\\sum_{i=1}^n y_i^2}}$$\n",
    "\n",
    "## Jaccard similarity\n",
    "\n",
    "This metric is a set similarity; that is, it only captures the presence and absence of terms with no regard to their frequency. Put simply, it captures the ratio of shared terms and total terms in the two documents.\n",
    "\n",
    "$$sim_{Jaccard} = \\frac{|X \\cap Y|}{|X \\cup Y|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def jaccard(x, y):\n",
    "    \"\"\"Computes the Jaccard similarity between two term vectors.\"\"\"\n",
    "    num_both = 0\n",
    "    num_either = 0\n",
    "    for xi, yi in zip(x, y):\n",
    "        num_both += int(xi > 0 and yi > 0)\n",
    "        num_either += int(xi > 0 or yi > 0)\n",
    "    return num_both / num_either\n",
    "\n",
    "def cosine(x, y):\n",
    "    \"\"\"Computes the Cosine similarity between two term vectors.\"\"\"\n",
    "    dot_product = 0\n",
    "    x_len = 0\n",
    "    y_len = 0\n",
    "    for xi, yi in zip(x, y):\n",
    "        dot_product += xi * yi\n",
    "        x_len += xi ** 2\n",
    "        y_len += yi ** 2\n",
    "        \n",
    "    return dot_product / (math.sqrt(x_len) * math.sqrt(y_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity : 0.9432422182837985\n",
      "jaccard similarity : 1.0\n"
     ]
    }
   ],
   "source": [
    "actual = [1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4]\n",
    "predicted = [1, 1, 1, 2, 3, 2, 3, 1, 3, 4, 2, 3]\n",
    "j = jaccard(actual, predicted)\n",
    "c = cosine(actual, predicted)\n",
    "print(f'cosine similarity : {c}')\n",
    "print(f'jaccard similarity : {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two father and two son went fish they saw a tall and strong tree reach high into the heaven for all the world to see they spent three hour there befor head home\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "sentence = '''\n",
    "Two fathers and two sons went fishing They saw a tall and strong tree \n",
    "reaching high into the heavens for all the world to see They spent three \n",
    "hours there before heading home'''\n",
    "stemmed = ' '.join(stemmer.stem(word) for word in sentence.split())\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classifier\n",
    "\n",
    "Create term/document or term/class matrix and compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "* lowercase\n",
    "* vocabulary according to stopwords\n",
    "* suffix-s stemming\n",
    "* posting lists for docs\n",
    "* frequency information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval evaluation\n",
    "\n",
    "## Binary relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary(rankings, ground_truth):\n",
    "    sum_p5, sum_p10, sum_ap, sum_rr = 0, 0, 0, 0\n",
    "\n",
    "    for qid, ranking in sorted(rankings.items()):\n",
    "        gt = gtruth[qid]    \n",
    "        print(\"Query\", qid, \"\\n\\tranking:\", ranking, \"\\n\\tground truth:\", gt)\n",
    "\n",
    "        p5, p10, ap, rr, num_rel = 0, 0, 0, 0, 0\n",
    "\n",
    "        for i, doc_id in enumerate(ranking):\n",
    "            if doc_id in gt:  # doc is relevant\n",
    "                num_rel += 1  \n",
    "                pi = num_rel / (i + 1)  # P@i\n",
    "                ap += pi  # AP\n",
    "\n",
    "                if i < 5:  # P@5\n",
    "                    p5 += 1\n",
    "                if i < 10:  # P@10\n",
    "                    p10 += 1\n",
    "                if rr == 0:  # Reciprocal rank\n",
    "                    rr = 1 / (i + 1)\n",
    "\n",
    "        p5 /= 5\n",
    "        p10 /= 10\n",
    "        ap /= len(gt)  # divide by the number of relevant documents\n",
    "        print(\"\\tP@5:\", round(p5, 3), \"\\n\\tP@10:\", round(p10, 3), \"\\n\\tAP:\", round(ap, 3), \"\\n\\tRR:\", round(rr, 3))\n",
    "\n",
    "\n",
    "        sum_p5 += p5\n",
    "        sum_p10 += p10\n",
    "        sum_ap += ap\n",
    "        sum_rr += rr\n",
    "\n",
    "    print(\"Average\")\n",
    "    print(\"\\tP@5:\", round(sum_p5 / len(rankings), 3), \"\\n\\tP@10:\", round(sum_p10 / len(rankings), 3), \n",
    "          \"\\n\\tMAP:\", round(sum_ap / len(rankings), 3), \"\\n\\tMRR:\", round(sum_rr / len(rankings), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query q1 \n",
      "\tranking: [1, 2, 4, 5, 3, 6, 9, 8, 10, 7] \n",
      "\tground truth: [1, 3]\n",
      "\tP@5: 0.4 \n",
      "\tP@10: 0.2 \n",
      "\tAP: 0.7 \n",
      "\tRR: 1.0\n",
      "Query q2 \n",
      "\tranking: [1, 2, 4, 5, 3, 9, 8, 6, 10, 7] \n",
      "\tground truth: [2, 4, 5, 6]\n",
      "\tP@5: 0.6 \n",
      "\tP@10: 0.4 \n",
      "\tAP: 0.604 \n",
      "\tRR: 0.5\n",
      "Query q3 \n",
      "\tranking: [1, 7, 4, 5, 3, 6, 9, 8, 10, 2] \n",
      "\tground truth: [7]\n",
      "\tP@5: 0.2 \n",
      "\tP@10: 0.1 \n",
      "\tAP: 0.5 \n",
      "\tRR: 0.5\n",
      "Average\n",
      "\tP@5: 0.4 \n",
      "\tP@10: 0.233 \n",
      "\tMAP: 0.601 \n",
      "\tMRR: 0.667\n"
     ]
    }
   ],
   "source": [
    "rankings = {\n",
    "    \"q1\": [1, 2, 4, 5, 3, 6, 9, 8, 10, 7],\n",
    "    \"q2\": [1, 2, 4, 5, 3, 9, 8, 6, 10, 7],\n",
    "    \"q3\": [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n",
    "}\n",
    "\n",
    "gtruth = {\n",
    "    \"q1\": [1, 3],\n",
    "    \"q2\": [2, 4, 5, 6],\n",
    "    \"q3\": [7]\n",
    "}\n",
    "\n",
    "evaluate_binary(rankings, gtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded relevance\n",
    "\n",
    "Discounted cumulative gain at rank p:\n",
    "$DCG_p = rel_1 + \\sum_{i=2}^p\\frac{rel_i}{\\log_2 i}$\n",
    "\n",
    "Normalized discounted cumulative gain at rank p:\n",
    "$NDCG_p = \\frac{DCG_p}{IDCG}$\n",
    "\n",
    "where IDCG is the DCG_p score of an idealized (perfect) ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(rel, p):\n",
    "    dcg = rel[0]\n",
    "    for i in range(1, min(p, len(rel))): \n",
    "        dcg += rel[i] / math.log2(i + 1)  # rank position is indexed from 1..\n",
    "    return dcg\n",
    "\n",
    "def evaluate_graded(rankings, ground_truth):\n",
    "    sum_ndcg5 = 0\n",
    "    sum_ndcg10 = 0\n",
    "\n",
    "    for qid, ranking in sorted(rankings.items()):\n",
    "        gt = gtruth[qid]    \n",
    "        print(f'Query {qid}')\n",
    "\n",
    "        gains = [] # holds corresponding relevance levels for the ranked docs\n",
    "        for doc_id in ranking: \n",
    "            gain = gt.get(doc_id, 0)\n",
    "            gains.append(gain)\n",
    "        print(f'\\tGains: {gains}')\n",
    "\n",
    "        # relevance levels of the idealized ranking\n",
    "        gain_ideal = sorted([v for _, v in gt.items()], reverse=True)\n",
    "        print(f'\\tIdeal gains: {gain_ideal}')\n",
    "\n",
    "        ndcg5 = dcg(gains, 5) / dcg(gain_ideal, 5)\n",
    "        ndcg10 = dcg(gains, 10) / dcg(gain_ideal, 10)\n",
    "        sum_ndcg5 += ndcg5\n",
    "        sum_ndcg10 += ndcg10\n",
    "\n",
    "        print(f'\\tDCG@5:{dcg(gains, 5):.3f} \\n\\tDCG@10: {dcg(gains, 10):.3f}')\n",
    "        print(f'\\tNDCG@5:{ndcg5:.3f} \\n\\tNDCG@10: {ndcg10:.3f}')\n",
    "\n",
    "    print('Average')\n",
    "    print(f'\\tNDCG@5: {sum_ndcg5 / len(rankings):.3f} \\n\\tNDCG@10: {sum_ndcg10 / len(rankings):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query q1\n",
      "\tGains: [1, 2, 0, 3, 0, 0, 0, 0, 0, 0]\n",
      "\tIdeal gains: [3, 2, 1]\n",
      "\tDCG@5:4.500 \n",
      "\tDCG@10: 4.500\n",
      "\tNDCG@5:0.799 \n",
      "\tNDCG@10: 0.799\n",
      "Query q2\n",
      "\tGains: [2, 1, 0, 3, 0, 0, 0, 1, 3, 0]\n",
      "\tIdeal gains: [3, 3, 2, 1, 1]\n",
      "\tDCG@5:4.500 \n",
      "\tDCG@10: 5.780\n",
      "\tNDCG@5:0.549 \n",
      "\tNDCG@10: 0.705\n",
      "Query q3\n",
      "\tGains: [3, 2, 3, 2, 0, 1, 0, 1, 0, 0]\n",
      "\tIdeal gains: [3, 3, 2, 2, 1, 1]\n",
      "\tDCG@5:7.893 \n",
      "\tDCG@10: 8.613\n",
      "\tNDCG@5:0.908 \n",
      "\tNDCG@10: 0.949\n",
      "Average\n",
      "\tNDCG@5: 0.752 \n",
      "\tNDCG@10: 0.818\n"
     ]
    }
   ],
   "source": [
    "rankings = {\n",
    "    \"q1\": [2, 1, 3, 4, 5, 6, 10, 7, 9, 8],\n",
    "    \"q2\": [1, 2, 9, 4, 5, 6, 7, 8, 3, 10],\n",
    "    \"q3\": [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n",
    "}\n",
    "gtruth = {\n",
    "    \"q1\": {4: 3, 1: 2, 2: 1},\n",
    "    \"q2\": {3: 3, 4: 3, 1: 2, 2: 1, 8: 1},\n",
    "    \"q3\": {1: 3, 4: 3, 7: 2, 5: 2, 6: 1, 8: 1}\n",
    "}\n",
    "evaluate_graded(rankings, gtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query rA\n",
      "\tGains: [0, 3, 0, 0, 2, 3, 1, 0, 0, 0]\n",
      "\tIdeal gains: [3, 3, 2, 1]\n",
      "\tDCG@5:3.861 \n",
      "\tDCG@10: 5.378\n",
      "\tNDCG@5:0.497 \n",
      "\tNDCG@10: 0.693\n",
      "Query rB\n",
      "\tGains: [1, 2, 3, 0, 0, 3, 0, 0, 0, 0]\n",
      "\tIdeal gains: [3, 3, 2, 1]\n",
      "\tDCG@5:4.893 \n",
      "\tDCG@10: 6.053\n",
      "\tNDCG@5:0.630 \n",
      "\tNDCG@10: 0.780\n",
      "Average\n",
      "\tNDCG@5: 0.564 \n",
      "\tNDCG@10: 0.736\n"
     ]
    }
   ],
   "source": [
    "rankings = {\n",
    "    \"rA\": [10, 7, 9, 8, 2, 1, 3, 4, 5, 6],\n",
    "    \"rB\": [3, 2, 1, 4, 5, 7, 8, 10, 9, 6],\n",
    "}\n",
    "gtruth = {\n",
    "    \"rA\": {1: 3, 7: 3, 2: 2, 3: 1},\n",
    "    \"rB\": {1: 3, 7: 3, 2: 2, 3: 1}\n",
    "}\n",
    "evaluate_graded(rankings, gtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc\n",
    "\n",
    "The formula for the number of handshakes possible at a party with n people is $\\frac{n*(n - 1)}{2}$.\n",
    "This is because each of the n people can shake hands with n - 1 people (they would not shake their own hand), and the handshake between two people is not counted twice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
